library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
testinga<-testing
testinga$diagnosis<-as.numeric(testing$diagnosis)-1
# Build three different models
set.seed(62433)
mod1 <- train(diagnosis ~.,method="gbm",data=training)
set.seed(62433)
mod2 <- train(diagnosis ~.,method="rf",data=training)
set.seed(62433)
mod3 <- train(diagnosis ~.,method="lda",data=training)
#Stack the predictions together using random forests ("rf").
# What is the resulting accuracy on the test set?
# Is it better or worse than each of the individual predictions?
# Predict on the testing set
pred1 <- predict(mod1,testing)
pred2 <- predict(mod2,testing)
pred3 <- predict(mod3,testing)
# Accuracies of each model
ctmod1_test<-table(testing$diagnosis, pred1)
accuracy_mod1<-sum(diag(ctmod1_test))/sum(ctmod1_test)
accuracy_mod1
ctmod2_test<-table(testing$diagnosis, pred2)
accuracy_mod2<-sum(diag(ctmod2_test))/sum(ctmod2_test)
accuracy_mod2
ctmod3_test<-table(testing$diagnosis, pred3)
accuracy_mod3<-sum(diag(ctmod3_test))/sum(ctmod3_test)
accuracy_mod3
# stack the models
dfStack<-data.frame(pred1,pred2,pred3,diagnosis=testing$diagnosis)
#fit a model using rf that combines the predictors
combModFit <- train(diagnosis ~.,method="rf",
data=dfStack)
combPred <- predict(combModFit,dfStack)
# combined accuracy
ct_comb<-table(testing$diagnosis, combPred)
accuracy_comb<-sum(diag(ct_comb))/sum(ct_comb)
accuracy_comb
accuracy_mod1
accuracy_mod2
accuracy_mod3
install.packages("e1071")
## QUESTION TWO
rm(list=ls())
#load Alzheimer's data
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
testinga<-testing
testinga$diagnosis<-as.numeric(testing$diagnosis)-1
# Build three different models
set.seed(62433)
mod1 <- train(diagnosis ~.,method="gbm",data=training)
set.seed(62433)
mod2 <- train(diagnosis ~.,method="rf",data=training)
set.seed(62433)
mod3 <- train(diagnosis ~.,method="lda",data=training)
#Stack the predictions together using random forests ("rf").
# What is the resulting accuracy on the test set?
# Is it better or worse than each of the individual predictions?
# Predict on the testing set
pred1 <- predict(mod1,testing)
pred2 <- predict(mod2,testing)
pred3 <- predict(mod3,testing)
# Accuracies of each model
ctmod1_test<-table(testing$diagnosis, pred1)
accuracy_mod1<-sum(diag(ctmod1_test))/sum(ctmod1_test)
accuracy_mod1
ctmod2_test<-table(testing$diagnosis, pred2)
accuracy_mod2<-sum(diag(ctmod2_test))/sum(ctmod2_test)
accuracy_mod2
ctmod3_test<-table(testing$diagnosis, pred3)
accuracy_mod3<-sum(diag(ctmod3_test))/sum(ctmod3_test)
accuracy_mod3
# stack the models
dfStack<-data.frame(pred1,pred2,pred3,diagnosis=testing$diagnosis)
#fit a model using rf that combines the predictors
combModFit <- train(diagnosis ~.,method="rf",
data=dfStack)
combPred <- predict(combModFit,dfStack)
# combined accuracy
ct_comb<-table(testing$diagnosis, combPred)
accuracy_comb<-sum(diag(ct_comb))/sum(ct_comb)
accuracy_comb
accuracy_mod1
accuracy_mod2
accuracy_mod3
rm(list=ls())
#load Alzheimer's data
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
testinga<-testing
testinga$diagnosis<-as.numeric(testing$diagnosis)-1
# Build three different models
set.seed(62433)
mod1 <- train(diagnosis ~.,method="gbm",data=training)
mod2 <- train(diagnosis ~.,method="rf",data=training)
mod3 <- train(diagnosis ~.,method="lda",data=training)
#Stack the predictions together using random forests ("rf").
# What is the resulting accuracy on the test set?
# Is it better or worse than each of the individual predictions?
# Predict on the testing set
pred1 <- predict(mod1,testing)
pred2 <- predict(mod2,testing)
pred3 <- predict(mod3,testing)
# Accuracies of each model
ctmod1_test<-table(testing$diagnosis, pred1)
accuracy_mod1<-sum(diag(ctmod1_test))/sum(ctmod1_test)
accuracy_mod1
ctmod2_test<-table(testing$diagnosis, pred2)
accuracy_mod2<-sum(diag(ctmod2_test))/sum(ctmod2_test)
accuracy_mod2
ctmod3_test<-table(testing$diagnosis, pred3)
accuracy_mod3<-sum(diag(ctmod3_test))/sum(ctmod3_test)
accuracy_mod3
# stack the models
dfStack<-data.frame(pred1,pred2,pred3,diagnosis=testing$diagnosis)
#fit a model using rf that combines the predictors
combModFit <- train(diagnosis ~.,method="rf",
data=dfStack)
combPred <- predict(combModFit,dfStack)
# combined accuracy
ct_comb<-table(testing$diagnosis, combPred)
accuracy_comb<-sum(diag(ct_comb))/sum(ct_comb)
accuracy_comb
accuracy_mod1
accuracy_mod2
accuracy_mod3
## QUESTION TWO
rm(list=ls())
#load Alzheimer's data
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
testinga<-testing
testinga$diagnosis<-as.numeric(testing$diagnosis)-1
# Build three different models
set.seed(62433)
mod1 <- train(diagnosis ~.,method="gbm",data=training)
set.seed(62433)
mod2 <- train(diagnosis ~.,method="rf",data=training)
set.seed(62433)
mod3 <- train(diagnosis ~.,method="lda",data=training)
#Stack the predictions together using random forests ("rf").
# What is the resulting accuracy on the test set?
# Is it better or worse than each of the individual predictions?
# Predict on the testing set
pred1 <- predict(mod1,testing)
pred2 <- predict(mod2,testing)
pred3 <- predict(mod3,testing)
# Accuracies of each model
ctmod1_test<-table(testing$diagnosis, pred1)
accuracy_mod1<-sum(diag(ctmod1_test))/sum(ctmod1_test)
accuracy_mod1
ctmod2_test<-table(testing$diagnosis, pred2)
accuracy_mod2<-sum(diag(ctmod2_test))/sum(ctmod2_test)
accuracy_mod2
ctmod3_test<-table(testing$diagnosis, pred3)
accuracy_mod3<-sum(diag(ctmod3_test))/sum(ctmod3_test)
accuracy_mod3
# stack the models
dfStack<-data.frame(pred1,pred2,pred3,diagnosis=testing$diagnosis)
#fit a model using rf that combines the predictors
combModFit <- train(diagnosis ~.,method="rf",
data=dfStack)
combPred <- predict(combModFit,dfStack)
# combined accuracy
ct_comb<-table(testing$diagnosis, combPred)
accuracy_comb<-sum(diag(ct_comb))/sum(ct_comb)
accuracy_comb
accuracy_mod1
accuracy_mod2
accuracy_mod3
## QUESTION TWO
rm(list=ls())
#load Alzheimer's data
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
testinga<-testing
testinga$diagnosis<-as.numeric(testing$diagnosis)-1
# Build three different models
set.seed(62433)
mod2 <- train(diagnosis ~.,method="rf",data=training)
mod1 <- train(diagnosis ~.,method="gbm",data=training)
mod3 <- train(diagnosis ~.,method="lda",data=training)
#Stack the predictions together using random forests ("rf").
# What is the resulting accuracy on the test set?
# Is it better or worse than each of the individual predictions?
# Predict on the testing set
pred1 <- predict(mod1,testing)
pred2 <- predict(mod2,testing)
pred3 <- predict(mod3,testing)
# Accuracies of each model
ctmod1_test<-table(testing$diagnosis, pred1)
accuracy_mod1<-sum(diag(ctmod1_test))/sum(ctmod1_test)
accuracy_mod1
ctmod2_test<-table(testing$diagnosis, pred2)
accuracy_mod2<-sum(diag(ctmod2_test))/sum(ctmod2_test)
accuracy_mod2
ctmod3_test<-table(testing$diagnosis, pred3)
accuracy_mod3<-sum(diag(ctmod3_test))/sum(ctmod3_test)
accuracy_mod3
# stack the models
dfStack<-data.frame(pred1,pred2,pred3,diagnosis=testing$diagnosis)
#fit a model using rf that combines the predictors
combModFit <- train(diagnosis ~.,method="rf",
data=dfStack)
combPred <- predict(combModFit,dfStack)
# combined accuracy
ct_comb<-table(testing$diagnosis, combPred)
accuracy_comb<-sum(diag(ct_comb))/sum(ct_comb)
accuracy_comb
accuracy_mod1
accuracy_mod2
accuracy_mod3
set.seed(62433)
combModFit <- train(diagnosis ~.,method="rf",
data=dfStack)
combPred <- predict(combModFit,dfStack)
# combined accuracy
ct_comb<-table(testing$diagnosis, combPred)
accuracy_comb<-sum(diag(ct_comb))/sum(ct_comb)
accuracy_comb
accuracy_mod1
accuracy_mod2
accuracy_mod3
#load Alzheimer's data
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
testinga<-testing
testinga$diagnosis<-as.numeric(testing$diagnosis)-1
# Build three different models
set.seed(62433)
mod2 <- train(diagnosis ~.,method="rf",data=training)
set.seed(62433)
mod1 <- train(diagnosis ~.,method="gbm",data=training)
mod3 <- train(diagnosis ~.,method="lda",data=training)
#Stack the predictions together using random forests ("rf").
# What is the resulting accuracy on the test set?
# Is it better or worse than each of the individual predictions?
# Predict on the testing set
pred1 <- predict(mod1,testing)
pred2 <- predict(mod2,testing)
pred3 <- predict(mod3,testing)
# Accuracies of each model
ctmod1_test<-table(testing$diagnosis, pred1)
accuracy_mod1<-sum(diag(ctmod1_test))/sum(ctmod1_test)
accuracy_mod1
ctmod2_test<-table(testing$diagnosis, pred2)
accuracy_mod2<-sum(diag(ctmod2_test))/sum(ctmod2_test)
accuracy_mod2
ctmod3_test<-table(testing$diagnosis, pred3)
accuracy_mod3<-sum(diag(ctmod3_test))/sum(ctmod3_test)
accuracy_mod3
# stack the models
dfStack<-data.frame(pred1,pred2,pred3,diagnosis=testing$diagnosis)
#fit a model using rf that combines the predictors
set.seed(62433)
combModFit <- train(diagnosis ~.,method="rf",
data=dfStack)
combPred <- predict(combModFit,dfStack)
# combined accuracy
ct_comb<-table(testing$diagnosis, combPred)
accuracy_comb<-sum(diag(ct_comb))/sum(ct_comb)
accuracy_comb
accuracy_mod1
accuracy_mod2
accuracy_mod3
rm(list=ls())
#load Alzheimer's data
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
testinga<-testing
testinga$diagnosis<-as.numeric(testing$diagnosis)-1
# Build three different models
set.seed(62433)
mod2 <- train(diagnosis ~.,method="rf",data=training)
mod1 <- train(diagnosis ~.,method="gbm",data=training)
mod3 <- train(diagnosis ~.,method="lda",data=training)
#Stack the predictions together using random forests ("rf").
# What is the resulting accuracy on the test set?
# Is it better or worse than each of the individual predictions?
# Predict on the testing set
pred1 <- predict(mod1,testing)
pred2 <- predict(mod2,testing)
pred3 <- predict(mod3,testing)
# Accuracies of each model
ctmod1_test<-table(testing$diagnosis, pred1)
accuracy_mod1<-sum(diag(ctmod1_test))/sum(ctmod1_test)
accuracy_mod1
ctmod2_test<-table(testing$diagnosis, pred2)
accuracy_mod2<-sum(diag(ctmod2_test))/sum(ctmod2_test)
accuracy_mod2
ctmod3_test<-table(testing$diagnosis, pred3)
accuracy_mod3<-sum(diag(ctmod3_test))/sum(ctmod3_test)
accuracy_mod3
# stack the models
dfStack<-data.frame(pred1,pred2,pred3,diagnosis=testing$diagnosis)
#fit a model using rf that combines the predictors
set.seed(62433)
combModFit <- train(diagnosis ~.,method="rf",
data=dfStack)
combPred <- predict(combModFit,dfStack)
# combined accuracy
ct_comb<-table(testing$diagnosis, combPred)
accuracy_comb<-sum(diag(ct_comb))/sum(ct_comb)
accuracy_comb
accuracy_mod1
accuracy_mod2
accuracy_mod3
#load Alzheimer's data
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
testinga<-testing
testinga$diagnosis<-as.numeric(testing$diagnosis)-1
# Build three different models
set.seed(62433)
mod2 <- train(diagnosis ~.,method="rf",data=training)
mod1 <- train(diagnosis ~.,method="gbm",data=training)
mod3 <- train(diagnosis ~.,method="lda",data=training)
#Stack the predictions together using random forests ("rf").
# What is the resulting accuracy on the test set?
# Is it better or worse than each of the individual predictions?
# Predict on the testing set
pred1 <- predict(mod1,testing)
pred2 <- predict(mod2,testing)
pred3 <- predict(mod3,testing)
# Accuracies of each model
ctmod1_test<-table(testing$diagnosis, pred1)
accuracy_mod1<-sum(diag(ctmod1_test))/sum(ctmod1_test)
accuracy_mod1
ctmod2_test<-table(testing$diagnosis, pred2)
accuracy_mod2<-sum(diag(ctmod2_test))/sum(ctmod2_test)
accuracy_mod2
ctmod3_test<-table(testing$diagnosis, pred3)
accuracy_mod3<-sum(diag(ctmod3_test))/sum(ctmod3_test)
accuracy_mod3
# stack the models
dfStack<-data.frame(pred1,pred2,pred3,diagnosis=testing$diagnosis)
#fit a model using rf that combines the predictors
combModFit <- train(diagnosis ~.,method="rf",
data=dfStack)
combPred <- predict(combModFit,dfStack)
# combined accuracy
ct_comb<-table(testing$diagnosis, combPred)
accuracy_comb<-sum(diag(ct_comb))/sum(ct_comb)
accuracy_comb
accuracy_mod1
accuracy_mod2
accuracy_mod3
rm(list=ls())
#load Alzheimer's data
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
testinga<-testing
testinga$diagnosis<-as.numeric(testing$diagnosis)-1
# Build three different models
set.seed(62433)
mod1 <- train(diagnosis ~.,method="gbm",data=training)
mod2 <- train(diagnosis ~.,method="rf",data=training)
mod3 <- train(diagnosis ~.,method="lda",data=training)
#Stack the predictions together using random forests ("rf").
# What is the resulting accuracy on the test set?
# Is it better or worse than each of the individual predictions?
# Predict on the testing set
pred1 <- predict(mod1,testing)
pred2 <- predict(mod2,testing)
pred3 <- predict(mod3,testing)
# Accuracies of each model
ctmod1_test<-table(testing$diagnosis, pred1)
accuracy_mod1<-sum(diag(ctmod1_test))/sum(ctmod1_test)
accuracy_mod1
ctmod2_test<-table(testing$diagnosis, pred2)
accuracy_mod2<-sum(diag(ctmod2_test))/sum(ctmod2_test)
accuracy_mod2
ctmod3_test<-table(testing$diagnosis, pred3)
accuracy_mod3<-sum(diag(ctmod3_test))/sum(ctmod3_test)
accuracy_mod3
# stack the models
dfStack<-data.frame(pred1,pred2,pred3,diagnosis=testing$diagnosis)
#fit a model using rf that combines the predictors
set.seed(62433)
combModFit <- train(diagnosis ~.,method="rf",
data=dfStack)
combPred <- predict(combModFit,dfStack)
# combined accuracy
ct_comb<-table(testing$diagnosis, combPred)
accuracy_comb<-sum(diag(ct_comb))/sum(ct_comb)
accuracy_comb
accuracy_mod1
accuracy_mod2
accuracy_mod3
shiny::runApp('H:/Rspace/JHU_Data_Science/JHU_DDP/Project')
shiny::runApp('H:/Rspace/shiny-examples/063-superzip-example')
rm(list=ls())
shiny::runApp('H:/Rspace/JHU_Data_Science/JHU_DDP/Project')
shiny::runApp('H:/Rspace/JHU_Data_Science/JHU_DDP/Project')
shiny::runApp('H:/Rspace/JHU_Data_Science/JHU_DDP/Project')
?radioButtons
shiny::runApp('H:/Rspace/JHU_Data_Science/JHU_DDP/Project')
a=2
b=3
if(a>b,5,6)
?if
shiny::runApp('H:/Rspace/JHU_Data_Science/JHU_DDP/Project')
shiny::runApp('H:/Rspace/JHU_Data_Science/JHU_DDP/Project')
rm(list=ls())
shiny::runApp('H:/Rspace/JHU_Data_Science/JHU_DDP/Project')
min(3,2)
shiny::runApp('H:/Rspace/JHU_Data_Science/JHU_DDP/Project')
shiny::runApp('H:/Rspace/JHU_Data_Science/JHU_DDP/Project')
shiny::runApp('H:/Rspace/JHU_Data_Science/JHU_DDP/Project')
shiny::runApp('H:/Rspace/JHU_Data_Science/JHU_DDP/Project')
shiny::runApp('H:/Rspace/JHU_Data_Science/JHU_DDP/Project')
shiny::runApp('H:/Rspace/JHU_Data_Science/JHU_DDP/Project')
shiny::runApp('H:/Rspace/JHU_Data_Science/JHU_DDP/Project')
shiny::runApp('H:/Rspace/JHU_Data_Science/JHU_DDP/Project')
shiny::runApp('H:/Rspace/JHU_Data_Science/JHU_DDP/Project')
shiny::runApp('H:/Rspace/JHU_Data_Science/JHU_DDP/Project')
shiny::runApp('H:/Rspace/JHU_Data_Science/JHU_DDP/Project')
shiny::runApp('H:/Rspace/JHU_Data_Science/JHU_DDP/Project')
a<-windSpeeds[,c(1:3,4)]
head(a)
b<-2
a<-windSpeeds[,c(1:3,3+b)]
head(a)
shiny::runApp('H:/Rspace/JHU_Data_Science/JHU_DDP/Project')
head(windSpeeds)
shiny::runApp('H:/Rspace/JHU_Data_Science/JHU_DDP/Project')
shiny::runApp('H:/Rspace/JHU_Data_Science/JHU_DDP/Project')
shiny::runApp('H:/Rspace/JHU_Data_Science/JHU_DDP/Project')
shiny::runApp('H:/Rspace/JHU_Data_Science/JHU_DDP/Project')
shiny::runApp('H:/Rspace/JHU_Data_Science/JHU_DDP/Project')
shiny::runApp('H:/Rspace/JHU_Data_Science/JHU_DDP/Project')
shiny::runApp('H:/Rspace/JHU_Data_Science/JHU_DDP/Project')
shiny::runApp('H:/Rspace/JHU_Data_Science/JHU_DDP/Project')
shiny::runApp('H:/Rspace/JHU_Data_Science/JHU_DDP/Project')
shiny::runApp('H:/Rspace/JHU_Data_Science/JHU_DDP/Project')
shiny::runApp('H:/Rspace/JHU_Data_Science/JHU_DDP/Project')
shiny::runApp('H:/Rspace/JHU_Data_Science/JHU_DDP/Project')
shiny::runApp('H:/Rspace/JHU_Data_Science/JHU_DDP/Project')
shiny::runApp('H:/Rspace/JHU_Data_Science/JHU_DDP/Project')
shiny::runApp('H:/Rspace/JHU_Data_Science/JHU_DDP/Project')
rm(list=ls())
shiny::runApp('H:/Rspace/JHU_Data_Science/JHU_DDP/Project')
shiny::runApp('H:/Rspace/JHU_Data_Science/JHU_DDP/Project')
shiny::runApp('H:/Rspace/JHU_Data_Science/JHU_DDP/Project')
shiny::runApp('H:/Rspace/JHU_Data_Science/JHU_DDP/Project')
shiny::runApp('H:/Rspace/JHU_Data_Science/JHU_DDP/Project')
shiny::runApp('H:/Rspace/JHU_Data_Science/JHU_DDP/Project')
shiny::runApp('H:/Rspace/JHU_Data_Science/JHU_DDP/Project')
shiny::runApp('H:/Rspace/JHU_Data_Science/JHU_DDP/Project')
